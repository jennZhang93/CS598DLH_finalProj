{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "start_memory = psutil.virtual_memory().available\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARS-CoV-2 Knowledge Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.utils import add_remaining_self_loops, add_self_loops\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import GCNConv, SAGEConv,GAE, VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='v0'\n",
    "device_id=0 #'cpu' if CPU, device number if GPU\n",
    "embedding_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))\n",
    "node_feature_np=pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature=torch.tensor(node_feature_np, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=torch.tensor(edge_index[['node1', 'node2']].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_dict={'gene-drug':0,'gene-gene':1,'bait-gene':2, 'gene-phenotype':3, 'drug-phenotype':4}\n",
    "edge_index['type']=edge_index['type'].apply(lambda x: edge_attr_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29759\n",
       "1     6214\n",
       "3     2195\n",
       "4     1526\n",
       "2      247\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr=torch.tensor(edge_index['type'].values,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=node_feature,\n",
    "            edge_index=edge.t().contiguous(),\n",
    "            edge_attr=edge_attr\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 15973, 39941)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_features, data.num_nodes,data.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39941])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.has_isolated_nodes(), data.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1):\n",
    "    r\"\"\"Splits the edges of a :obj:`torch_geometric.data.Data` object\n",
    "    into positive and negative train/val/test edges, and adds attributes of\n",
    "    `train_pos_edge_index`, `train_neg_adj_mask`, `val_pos_edge_index`,\n",
    "    `val_neg_edge_index`, `test_pos_edge_index`, and `test_neg_edge_index`\n",
    "    to :attr:`data`.\n",
    "\n",
    "    Args:\n",
    "        data (Data): The data object.\n",
    "        val_ratio (float, optional): The ratio of positive validation\n",
    "            edges. (default: :obj:`0.05`)\n",
    "        test_ratio (float, optional): The ratio of positive test\n",
    "            edges. (default: :obj:`0.1`)\n",
    "\n",
    "    :rtype: :class:`torch_geometric.data.Data`\n",
    "    \"\"\"\n",
    "\n",
    "    assert 'batch' not in data  # No batch-mode.\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    row, col = data.edge_index\n",
    "    #data.edge_index = None\n",
    "    attr = data.edge_attr\n",
    "\n",
    "    # Return upper triangular portion.\n",
    "    #mask = row < col\n",
    "    #row, col = row[mask], col[mask]\n",
    "\n",
    "    n_v = int(math.floor(val_ratio * row.size(0)))\n",
    "    n_t = int(math.floor(test_ratio * row.size(0)))\n",
    "\n",
    "    # Positive edges.\n",
    "    perm = torch.randperm(row.size(0))\n",
    "    row, col = row[perm], col[perm]\n",
    "    attr=attr[perm]\n",
    "\n",
    "    r, c = row[:n_v], col[:n_v]\n",
    "    data.val_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.val_pos_edge_attr = attr[:n_v]\n",
    "    \n",
    "    r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\n",
    "    data.test_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.test_post_edge_attr = attr[n_v:n_v + n_t]\n",
    "\n",
    "    r, c = row[n_v + n_t:], col[n_v + n_t:]\n",
    "    data.train_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.train_pos_edge_attr = attr[n_v+n_t:]\n",
    "\n",
    "    # Negative edges.\n",
    "    neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)\n",
    "    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "    neg_adj_mask[row, col] = 0\n",
    "\n",
    "    neg_row, neg_col = neg_adj_mask.nonzero().t()\n",
    "    perm = random.sample(range(neg_row.size(0)),\n",
    "                         min(n_v + n_t, neg_row.size(0)))\n",
    "    perm = torch.tensor(perm)\n",
    "    perm = perm.to(torch.long)\n",
    "    neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "\n",
    "    neg_adj_mask[neg_row, neg_col] = 0\n",
    "    data.train_neg_adj_mask = neg_adj_mask\n",
    "\n",
    "    row, col = neg_row[:n_v], neg_col[:n_v]\n",
    "    data.val_neg_edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    row, col = neg_row[n_v:n_v + n_t], neg_col[n_v:n_v + n_t]\n",
    "    data.test_neg_edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=train_test_split_edges(data, test_ratio=0.1, val_ratio=0)\n",
    "x,train_pos_edge_index,train_pos_edge_attr = data_split.x.to(device), data_split.train_pos_edge_index.to(device), data_split.train_pos_edge_attr.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_edge_index, train_pos_edge_attr=add_remaining_self_loops(train_pos_edge_index,train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26802\n",
       "1    21497\n",
       "3     1966\n",
       "4     1378\n",
       "2      227\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_pos_edge_attr.cpu().numpy()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,train_pos_edge_index,train_pos_edge_attr = Variable(x),Variable(train_pos_edge_index),Variable(train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VGAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_VGAE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, isClassificationTask=False):\n",
    "        super(Encoder_VGAE, self).__init__()\n",
    "        self.isClassificationTask=isClassificationTask\n",
    "        self.conv_gene_drug=  SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_gene_gene = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_bait_gene = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_gene_phenotype = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_drug_phenotype = SAGEConv(in_channels, 2*out_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(5*2*out_channels)\n",
    "        #variational encoder\n",
    "        self.conv_mu = SAGEConv(5*2*out_channels, out_channels, )\n",
    "        self.conv_logvar = SAGEConv(5*2*out_channels, out_channels,)\n",
    "\n",
    "    def forward(self,x,edge_index,edge_attr):\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        index_gene_drug=(edge_attr==0).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_drug=edge_index[:, index_gene_drug]\n",
    "        \n",
    "        index_gene_gene=(edge_attr==1).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_gene=edge_index[:, index_gene_gene]\n",
    "        \n",
    "        index_bait_gene=(edge_attr==2).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_bait_gene=edge_index[:, index_bait_gene]\n",
    "        \n",
    "        index_gene_phenotype=(edge_attr==3).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_phenotype=edge_index[:, index_gene_phenotype]\n",
    "        \n",
    "        index_drug_phenotype=(edge_attr==4).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_drug_phenotype=edge_index[:, index_drug_phenotype]\n",
    "        \n",
    "        \n",
    "        x_gene_drug = F.dropout(F.relu(self.conv_gene_drug(x,edge_index_gene_drug)), p=0.5, training=self.training, )\n",
    "        x_gene_gene = F.dropout(F.relu(self.conv_gene_gene(x,edge_index_gene_gene)), p=0.5, training=self.training)\n",
    "        x_bait_gene = F.dropout(F.relu(self.conv_bait_gene(x,edge_index_bait_gene)), p=0.1, training=self.training)\n",
    "        x_gene_phenotype = F.dropout(F.relu(self.conv_gene_phenotype(x,edge_index_gene_phenotype)), training=self.training)\n",
    "        x_drug_phenotype = F.dropout(F.relu(self.conv_drug_phenotype(x,edge_index_drug_phenotype)), training=self.training)\n",
    "\n",
    "        x=self.bn(torch.cat([x_gene_drug,x_gene_gene,x_bait_gene,x_gene_phenotype,x_drug_phenotype],dim=1))        \n",
    "        \n",
    "        return self.conv_mu(x,edge_index), self.conv_logvar(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGAE(Encoder_VGAE(node_feature.shape[1], embedding_size)).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index, train_pos_edge_attr)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    \n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z=model.encode(x, train_pos_edge_index,train_pos_edge_attr)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3815111751252317, 0.48391196894675925)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DRKG's accuracy for comparison\n",
    "model.test(x,data_split.test_pos_edge_index, data_split.test_neg_edge_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.96141815185547\n",
      "Epoch: 001, AUC: 0.6213, AP: 0.5792\n",
      "17.61661148071289\n",
      "Epoch: 002, AUC: 0.6644, AP: 0.5916\n",
      "16.693666458129883\n",
      "Epoch: 003, AUC: 0.7029, AP: 0.6329\n",
      "16.586679458618164\n",
      "Epoch: 004, AUC: 0.7279, AP: 0.6674\n",
      "16.506986618041992\n",
      "Epoch: 005, AUC: 0.7503, AP: 0.7002\n",
      "16.771818161010742\n",
      "Epoch: 006, AUC: 0.7681, AP: 0.7268\n",
      "16.495819091796875\n",
      "Epoch: 007, AUC: 0.7829, AP: 0.7351\n",
      "16.0690975189209\n",
      "Epoch: 008, AUC: 0.7890, AP: 0.7373\n",
      "15.337748527526855\n",
      "Epoch: 009, AUC: 0.7835, AP: 0.7180\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    auc, ap = test(data_split.test_pos_edge_index, data_split.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z=model.encode(x, data.edge_index.to(device), data.edge_attr.to(device))\n",
    "z_np = z.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(z_np, open(data_path+'node_embedding_'+exp_id+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path+'VAE_encoders_'+exp_id+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): Encoder_VGAE(\n",
       "    (conv_gene_drug): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_gene_gene): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_bait_gene): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_gene_phenotype): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_drug_phenotype): SAGEConv(400, 256, aggr=mean)\n",
       "    (bn): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_mu): SAGEConv(1280, 128, aggr=mean)\n",
       "    (conv_logvar): SAGEConv(1280, 128, aggr=mean)\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(data_path+'VAE_encoders_'+exp_id+'.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk=300\n",
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load drugs under clinical trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR loss NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim=embedding_size):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_size).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.8528066873550415\n",
      "test loss 0.6282595992088318\n",
      "training loss 0.6768390536308289\n",
      "test loss 0.625196635723114\n",
      "training loss 0.6312181353569031\n",
      "test loss 0.6167804598808289\n",
      "training loss 0.5605171918869019\n",
      "test loss 0.6301008462905884\n",
      "training loss 0.48370474576950073\n",
      "test loss 0.616718590259552\n",
      "training loss 0.5584940314292908\n",
      "test loss 0.6221783757209778\n",
      "training loss 0.512782871723175\n",
      "test loss 0.6086599230766296\n",
      "training loss 0.5476498603820801\n",
      "test loss 0.6092787981033325\n",
      "training loss 0.5959728360176086\n",
      "test loss 0.5850163698196411\n",
      "training loss 0.603988766670227\n",
      "test loss 0.5835478901863098\n",
      "training loss 0.5041338801383972\n",
      "test loss 0.5839903354644775\n",
      "training loss 0.5230756402015686\n",
      "test loss 0.5763224959373474\n",
      "training loss 0.6181591153144836\n",
      "test loss 0.5699707865715027\n",
      "training loss 0.5893479585647583\n",
      "test loss 0.5793527364730835\n",
      "training loss 0.5203702449798584\n",
      "test loss 0.587904155254364\n",
      "training loss 0.5849248766899109\n",
      "test loss 0.5475890636444092\n",
      "training loss 0.5164147615432739\n",
      "test loss 0.5707100033760071\n",
      "training loss 0.573073148727417\n",
      "test loss 0.5607728958129883\n",
      "training loss 0.5216575264930725\n",
      "test loss 0.5604519844055176\n",
      "training loss 0.6004493236541748\n",
      "test loss 0.5586333870887756\n",
      "training loss 0.5115736722946167\n",
      "test loss 0.5673378705978394\n",
      "training loss 0.5400705337524414\n",
      "test loss 0.540961503982544\n",
      "training loss 0.47467148303985596\n",
      "test loss 0.5680919885635376\n",
      "training loss 0.5833852887153625\n",
      "test loss 0.5525545477867126\n",
      "training loss 0.4923708140850067\n",
      "test loss 0.5472676157951355\n",
      "training loss 0.496355801820755\n",
      "test loss 0.5412687063217163\n",
      "training loss 0.5248748064041138\n",
      "test loss 0.5321319699287415\n",
      "training loss 0.43552523851394653\n",
      "test loss 0.5311405062675476\n",
      "training loss 0.5452936887741089\n",
      "test loss 0.5472444891929626\n",
      "training loss 0.48038944602012634\n",
      "test loss 0.5400193333625793\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf-temp.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf-temp.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8899978950322762\n",
      "AUPRC 0.26913818201592205\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_idx=np.argsort(-clf(torch.tensor(z_np[types=='drug'],dtype=torch.float).to(device)).squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8793853494246421\n",
      "Logit AUPRC 0.22802852921545358\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.835391523996632\n",
      "XGBoost AUPRC 0.14278706501164526\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8712022873982599\n",
      "rf AUPRC 0.18812819774769082\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.7064183974179064\n",
      "svm AUPRC 0.17093736588994693\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_items_idx=np.argsort(-clf.predict_proba(z_np[types=='drug'])[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the high-ranked drugs into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_drugs=pd.DataFrame([(rank, drug.split('_')[1]) for rank,drug in enumerate(le.inverse_transform((types=='drug').nonzero()[0][top_items_idx])[:topk+1])], columns=['rank', 'drug'])\n",
    "topk_drugs['under_trials']=topk_drugs['drug'].isin(trials_drug).astype(int)\n",
    "topk_drugs['is_used_in_training']=topk_drugs['drug'].isin(np.array([drug.split('_')[1] for drug in le.classes_[types=='drug']])[indices_train]).astype(int)\n",
    "topk_drugs.to_csv('top300_drugs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 20.26 sec\n",
      "Memory usage: 2555.98 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "gpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'GPU time: {gpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
