{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "start_memory = psutil.virtual_memory().available\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARS-CoV-2 Knowledge Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.utils import add_remaining_self_loops, add_self_loops\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import GCNConv, SAGEConv,GAE, VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='v0'\n",
    "device_id=0 #'cpu' if CPU, device number if GPU\n",
    "embedding_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))\n",
    "node_feature_np=pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature=torch.tensor(node_feature_np, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=torch.tensor(edge_index[['node1', 'node2']].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_dict={'gene-drug':0,'gene-gene':1,'bait-gene':2, 'gene-phenotype':3, 'drug-phenotype':4}\n",
    "edge_index['type']=edge_index['type'].apply(lambda x: edge_attr_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29759\n",
       "1     6213\n",
       "3     2195\n",
       "4     1526\n",
       "2      247\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr=torch.tensor(edge_index['type'].values,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=node_feature,\n",
    "            edge_index=edge.t().contiguous(),\n",
    "            edge_attr=edge_attr\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 15973, 39940)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_features, data.num_nodes,data.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39940])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.has_isolated_nodes(), data.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1):\n",
    "    r\"\"\"Splits the edges of a :obj:`torch_geometric.data.Data` object\n",
    "    into positive and negative train/val/test edges, and adds attributes of\n",
    "    `train_pos_edge_index`, `train_neg_adj_mask`, `val_pos_edge_index`,\n",
    "    `val_neg_edge_index`, `test_pos_edge_index`, and `test_neg_edge_index`\n",
    "    to :attr:`data`.\n",
    "\n",
    "    Args:\n",
    "        data (Data): The data object.\n",
    "        val_ratio (float, optional): The ratio of positive validation\n",
    "            edges. (default: :obj:`0.05`)\n",
    "        test_ratio (float, optional): The ratio of positive test\n",
    "            edges. (default: :obj:`0.1`)\n",
    "\n",
    "    :rtype: :class:`torch_geometric.data.Data`\n",
    "    \"\"\"\n",
    "\n",
    "    assert 'batch' not in data  # No batch-mode.\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    row, col = data.edge_index\n",
    "    #data.edge_index = None\n",
    "    attr = data.edge_attr\n",
    "\n",
    "    # Return upper triangular portion.\n",
    "    #mask = row < col\n",
    "    #row, col = row[mask], col[mask]\n",
    "\n",
    "    n_v = int(math.floor(val_ratio * row.size(0)))\n",
    "    n_t = int(math.floor(test_ratio * row.size(0)))\n",
    "\n",
    "    # Positive edges.\n",
    "    perm = torch.randperm(row.size(0))\n",
    "    row, col = row[perm], col[perm]\n",
    "    attr=attr[perm]\n",
    "\n",
    "    r, c = row[:n_v], col[:n_v]\n",
    "    data.val_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.val_pos_edge_attr = attr[:n_v]\n",
    "    \n",
    "    r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\n",
    "    data.test_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.test_post_edge_attr = attr[n_v:n_v + n_t]\n",
    "\n",
    "    r, c = row[n_v + n_t:], col[n_v + n_t:]\n",
    "    data.train_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.train_pos_edge_attr = attr[n_v+n_t:]\n",
    "\n",
    "    # Negative edges.\n",
    "    neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)\n",
    "    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "    neg_adj_mask[row, col] = 0\n",
    "\n",
    "    neg_row, neg_col = neg_adj_mask.nonzero().t()\n",
    "    perm = random.sample(range(neg_row.size(0)),\n",
    "                         min(n_v + n_t, neg_row.size(0)))\n",
    "    perm = torch.tensor(perm)\n",
    "    perm = perm.to(torch.long)\n",
    "    neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "\n",
    "    neg_adj_mask[neg_row, neg_col] = 0\n",
    "    data.train_neg_adj_mask = neg_adj_mask\n",
    "\n",
    "    row, col = neg_row[:n_v], neg_col[:n_v]\n",
    "    data.val_neg_edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    row, col = neg_row[n_v:n_v + n_t], neg_col[n_v:n_v + n_t]\n",
    "    data.test_neg_edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=train_test_split_edges(data, test_ratio=0.1, val_ratio=0)\n",
    "x,train_pos_edge_index,train_pos_edge_attr = data_split.x.to(device), data_split.train_pos_edge_index.to(device), data_split.train_pos_edge_attr.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_edge_index, train_pos_edge_attr=add_remaining_self_loops(train_pos_edge_index,train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26783\n",
       "1    21514\n",
       "3     1988\n",
       "4     1368\n",
       "2      216\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_pos_edge_attr.cpu().numpy()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,train_pos_edge_index,train_pos_edge_attr = Variable(x),Variable(train_pos_edge_index),Variable(train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VGAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_VGAE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, isClassificationTask=False):\n",
    "        super(Encoder_VGAE, self).__init__()\n",
    "        self.isClassificationTask=isClassificationTask\n",
    "        self.conv_gene_drug=  SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_gene_gene = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_bait_gene = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_gene_phenotype = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_drug_phenotype = SAGEConv(in_channels, 2*out_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(5*2*out_channels)\n",
    "        #variational encoder\n",
    "        self.conv_mu = SAGEConv(5*2*out_channels, out_channels, )\n",
    "        self.conv_logvar = SAGEConv(5*2*out_channels, out_channels,)\n",
    "\n",
    "    def forward(self,x,edge_index,edge_attr):\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        index_gene_drug=(edge_attr==0).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_drug=edge_index[:, index_gene_drug]\n",
    "        \n",
    "        index_gene_gene=(edge_attr==1).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_gene=edge_index[:, index_gene_gene]\n",
    "        \n",
    "        index_bait_gene=(edge_attr==2).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_bait_gene=edge_index[:, index_bait_gene]\n",
    "        \n",
    "        index_gene_phenotype=(edge_attr==3).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_phenotype=edge_index[:, index_gene_phenotype]\n",
    "        \n",
    "        index_drug_phenotype=(edge_attr==4).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_drug_phenotype=edge_index[:, index_drug_phenotype]\n",
    "        \n",
    "        \n",
    "        x_gene_drug = F.dropout(F.relu(self.conv_gene_drug(x,edge_index_gene_drug)), p=0.5, training=self.training, )\n",
    "        x_gene_gene = F.dropout(F.relu(self.conv_gene_gene(x,edge_index_gene_gene)), p=0.5, training=self.training)\n",
    "        x_bait_gene = F.dropout(F.relu(self.conv_bait_gene(x,edge_index_bait_gene)), p=0.1, training=self.training)\n",
    "        x_gene_phenotype = F.dropout(F.relu(self.conv_gene_phenotype(x,edge_index_gene_phenotype)), training=self.training)\n",
    "        x_drug_phenotype = F.dropout(F.relu(self.conv_drug_phenotype(x,edge_index_drug_phenotype)), training=self.training)\n",
    "\n",
    "        x=self.bn(torch.cat([x_gene_drug,x_gene_gene,x_bait_gene,x_gene_phenotype,x_drug_phenotype],dim=1))        \n",
    "        \n",
    "        return self.conv_mu(x,edge_index), self.conv_logvar(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGAE(Encoder_VGAE(node_feature.shape[1], embedding_size)).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index, train_pos_edge_attr)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    \n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z=model.encode(x, train_pos_edge_index,train_pos_edge_attr)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37458516267139813, 0.48293354099607083)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DRKG's accuracy for comparison\n",
    "model.test(x,data_split.test_pos_edge_index, data_split.test_neg_edge_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.1021728515625\n",
      "Epoch: 001, AUC: 0.6347, AP: 0.5661\n",
      "18.0845947265625\n",
      "Epoch: 002, AUC: 0.6846, AP: 0.6027\n",
      "17.017955780029297\n",
      "Epoch: 003, AUC: 0.7265, AP: 0.6625\n",
      "16.29204559326172\n",
      "Epoch: 004, AUC: 0.7538, AP: 0.7130\n",
      "16.742753982543945\n",
      "Epoch: 005, AUC: 0.7717, AP: 0.7493\n",
      "16.703859329223633\n",
      "Epoch: 006, AUC: 0.7866, AP: 0.7724\n",
      "16.597787857055664\n",
      "Epoch: 007, AUC: 0.7976, AP: 0.7834\n",
      "16.140380859375\n",
      "Epoch: 008, AUC: 0.8003, AP: 0.7619\n",
      "15.195137977600098\n",
      "Epoch: 009, AUC: 0.7994, AP: 0.7394\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    auc, ap = test(data_split.test_pos_edge_index, data_split.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z=model.encode(x, data.edge_index.to(device), data.edge_attr.to(device))\n",
    "z_np = z.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(z_np, open(data_path+'node_embedding_'+exp_id+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path+'VAE_encoders_'+exp_id+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): Encoder_VGAE(\n",
       "    (conv_gene_drug): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_gene_gene): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_bait_gene): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_gene_phenotype): SAGEConv(400, 256, aggr=mean)\n",
       "    (conv_drug_phenotype): SAGEConv(400, 256, aggr=mean)\n",
       "    (bn): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_mu): SAGEConv(1280, 128, aggr=mean)\n",
       "    (conv_logvar): SAGEConv(1280, 128, aggr=mean)\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(data_path+'VAE_encoders_'+exp_id+'.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk=300\n",
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load drugs under clinical trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR loss NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim=embedding_size):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_size).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.8579958081245422\n",
      "test loss 0.6572800874710083\n",
      "training loss 0.6898871660232544\n",
      "test loss 0.6307414174079895\n",
      "training loss 0.5770067572593689\n",
      "test loss 0.6071283221244812\n",
      "training loss 0.5720771551132202\n",
      "test loss 0.5882228016853333\n",
      "training loss 0.6062000393867493\n",
      "test loss 0.5687003135681152\n",
      "training loss 0.5904142260551453\n",
      "test loss 0.5615933537483215\n",
      "training loss 0.6048142313957214\n",
      "test loss 0.5583937168121338\n",
      "training loss 0.5592907071113586\n",
      "test loss 0.541763186454773\n",
      "training loss 0.7188544869422913\n",
      "test loss 0.5555068254470825\n",
      "training loss 0.5936294198036194\n",
      "test loss 0.5585616827011108\n",
      "training loss 0.5686233639717102\n",
      "test loss 0.5118375420570374\n",
      "training loss 0.5560240149497986\n",
      "test loss 0.5400386452674866\n",
      "training loss 0.6424142122268677\n",
      "test loss 0.5317306518554688\n",
      "training loss 0.5615624189376831\n",
      "test loss 0.5340401530265808\n",
      "training loss 0.560788631439209\n",
      "test loss 0.5618463158607483\n",
      "training loss 0.5630025267601013\n",
      "test loss 0.515371561050415\n",
      "training loss 0.5791270136833191\n",
      "test loss 0.5050615072250366\n",
      "training loss 0.518434464931488\n",
      "test loss 0.5170922875404358\n",
      "training loss 0.534670889377594\n",
      "test loss 0.5098331570625305\n",
      "training loss 0.598561704158783\n",
      "test loss 0.47813767194747925\n",
      "training loss 0.5473285913467407\n",
      "test loss 0.5029891729354858\n",
      "training loss 0.488364040851593\n",
      "test loss 0.4898156523704529\n",
      "training loss 0.4663946032524109\n",
      "test loss 0.498393714427948\n",
      "training loss 0.5038986206054688\n",
      "test loss 0.5103658437728882\n",
      "training loss 0.5185754299163818\n",
      "test loss 0.47424447536468506\n",
      "training loss 0.48582401871681213\n",
      "test loss 0.4842316806316376\n",
      "training loss 0.538305401802063\n",
      "test loss 0.4884425699710846\n",
      "training loss 0.4731763005256653\n",
      "test loss 0.46402421593666077\n",
      "training loss 0.5103051662445068\n",
      "test loss 0.47210970520973206\n",
      "training loss 0.4457973539829254\n",
      "test loss 0.4721594452857971\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf-temp.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf-temp.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8936377350547291\n",
      "AUPRC 0.27530869417397363\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_idx=np.argsort(-clf(torch.tensor(z_np[types=='drug'],dtype=torch.float).to(device)).squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8821393488633174\n",
      "Logit AUPRC 0.22821921459851136\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8612343881560482\n",
      "XGBoost AUPRC 0.18144913246631086\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8816218776312096\n",
      "rf AUPRC 0.22396393783724947\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.7253367948358126\n",
      "svm AUPRC 0.1918917948296393\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_items_idx=np.argsort(-clf.predict_proba(z_np[types=='drug'])[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the high-ranked drugs into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_drugs=pd.DataFrame([(rank, drug.split('_')[1]) for rank,drug in enumerate(le.inverse_transform((types=='drug').nonzero()[0][top_items_idx])[:topk+1])], columns=['rank', 'drug'])\n",
    "topk_drugs['under_trials']=topk_drugs['drug'].isin(trials_drug).astype(int)\n",
    "topk_drugs['is_used_in_training']=topk_drugs['drug'].isin(np.array([drug.split('_')[1] for drug in le.classes_[types=='drug']])[indices_train]).astype(int)\n",
    "topk_drugs.to_csv('top300_drugs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU time: 20.67 sec\n",
      "Memory usage: 2515.71 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "gpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'GPU time: {gpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRKG embedding alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_np = pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 400\n",
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.7767941355705261\n",
      "test loss 0.5912396907806396\n",
      "training loss 0.4748830497264862\n",
      "test loss 0.5170939564704895\n",
      "training loss 0.46429628133773804\n",
      "test loss 0.5005640387535095\n",
      "training loss 0.5378431677818298\n",
      "test loss 0.4702387750148773\n",
      "training loss 0.5428016185760498\n",
      "test loss 0.44512736797332764\n",
      "training loss 0.522895336151123\n",
      "test loss 0.4604524075984955\n",
      "training loss 0.4853673577308655\n",
      "test loss 0.44241708517074585\n",
      "training loss 0.435994952917099\n",
      "test loss 0.4518546164035797\n",
      "training loss 0.3985897898674011\n",
      "test loss 0.4512467384338379\n",
      "training loss 0.4665781557559967\n",
      "test loss 0.433594286441803\n",
      "training loss 0.41058462858200073\n",
      "test loss 0.4388464093208313\n",
      "training loss 0.3483089208602905\n",
      "test loss 0.449398398399353\n",
      "training loss 0.3580435812473297\n",
      "test loss 0.4310453534126282\n",
      "training loss 0.3288162648677826\n",
      "test loss 0.4353797137737274\n",
      "training loss 0.3130916357040405\n",
      "test loss 0.44733133912086487\n",
      "training loss 0.3010924756526947\n",
      "test loss 0.45447322726249695\n",
      "training loss 0.2999395728111267\n",
      "test loss 0.44085177779197693\n",
      "training loss 0.3119945228099823\n",
      "test loss 0.4563553035259247\n",
      "training loss 0.3135868012905121\n",
      "test loss 0.4533992409706116\n",
      "training loss 0.3042580783367157\n",
      "test loss 0.4389675557613373\n",
      "training loss 0.32912787795066833\n",
      "test loss 0.45111942291259766\n",
      "training loss 0.34989577531814575\n",
      "test loss 0.42275160551071167\n",
      "training loss 0.32585766911506653\n",
      "test loss 0.43594157695770264\n",
      "training loss 0.2773635685443878\n",
      "test loss 0.40432214736938477\n",
      "training loss 0.3458378314971924\n",
      "test loss 0.373360276222229\n",
      "training loss 0.27897515892982483\n",
      "test loss 0.40167322754859924\n",
      "training loss 0.28520944714546204\n",
      "test loss 0.3977036476135254\n",
      "training loss 0.275092214345932\n",
      "test loss 0.40075212717056274\n",
      "training loss 0.3138018548488617\n",
      "test loss 0.3594669997692108\n",
      "training loss 0.30122512578964233\n",
      "test loss 0.40009599924087524\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf_DRKG_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf_DRKG_embedding.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8768593881560482\n",
      "AUPRC 0.1573396031066143\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 4.60 sec\n",
      "Memory usage: 55.37 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.738466531013191\n",
      "Logit AUPRC 0.12016480094609217\n",
      "XGBoost AUROC 0.7314236598372158\n",
      "XGBoost AUPRC 0.08611963269569185\n",
      "rf AUROC 0.7961821147909065\n",
      "rf AUPRC 0.08916341533273413\n",
      "svm AUROC 0.7976512068481619\n",
      "svm AUPRC 0.16243353839176214\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'DRKG alone',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 6.85 sec\n",
      "Memory usage: -1.84 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hybrid embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "embedding_size = 128\n",
    "exp_id='v0'\n",
    "node_feature_np=pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))\n",
    "node_feature=torch.tensor(node_feature_np, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=node_feature,\n",
    "            edge_index=edge.t().contiguous(),\n",
    "            edge_attr=edge_attr\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=train_test_split_edges(data, test_ratio=0.1, val_ratio=0)\n",
    "x, train_pos_edge_index, train_pos_edge_attr = data_split.x.to(device), \\\n",
    "    data_split.train_pos_edge_index.to(device), data_split.train_pos_edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_edge_index, train_pos_edge_attr=add_remaining_self_loops(train_pos_edge_index,train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,train_pos_edge_index,train_pos_edge_attr = Variable(x),Variable(train_pos_edge_index),Variable(train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGAE(Encoder_VGAE(node_feature.shape[1], embedding_size)).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3782639720722797, 0.4822463469615502)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DRKG's accuracy for comparison\n",
    "model.test(x,data_split.test_pos_edge_index, data_split.test_neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.289628982543945\n",
      "Epoch: 001, AUC: 0.6156, AP: 0.5555\n",
      "17.520038604736328\n",
      "Epoch: 002, AUC: 0.6803, AP: 0.6056\n",
      "16.64708709716797\n",
      "Epoch: 003, AUC: 0.7340, AP: 0.6882\n",
      "16.492076873779297\n",
      "Epoch: 004, AUC: 0.7526, AP: 0.7106\n",
      "16.301828384399414\n",
      "Epoch: 005, AUC: 0.7599, AP: 0.7145\n",
      "16.912879943847656\n",
      "Epoch: 006, AUC: 0.7708, AP: 0.7305\n",
      "16.47847557067871\n",
      "Epoch: 007, AUC: 0.7840, AP: 0.7351\n",
      "15.922550201416016\n",
      "Epoch: 008, AUC: 0.7898, AP: 0.7288\n",
      "15.020065307617188\n",
      "Epoch: 009, AUC: 0.7842, AP: 0.7082\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    auc, ap = test(data_split.test_pos_edge_index, data_split.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z=model.encode(x, data.edge_index.to(device), data.edge_attr.to(device))\n",
    "z_np = z.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(z_np, open(data_path+f'hybrid_embedding_{embedding_size}_'+exp_id+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path+f'hybrid_VAE_encoders_{embedding_size}_'+exp_id+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 2.13 sec\n",
      "Memory usage: -37.14 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_np = pickle.load(open(data_path+f'hybrid_embedding_{embed_dim}_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(embedding_dim=embed_dim).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.9196394681930542\n",
      "test loss 0.6824237704277039\n",
      "training loss 0.7809790372848511\n",
      "test loss 0.6407840251922607\n",
      "training loss 0.6653259992599487\n",
      "test loss 0.6323705315589905\n",
      "training loss 0.5585138201713562\n",
      "test loss 0.6368915438652039\n",
      "training loss 0.4999552369117737\n",
      "test loss 0.6383573412895203\n",
      "training loss 0.5141431093215942\n",
      "test loss 0.6211481094360352\n",
      "training loss 0.5294291377067566\n",
      "test loss 0.6180371046066284\n",
      "training loss 0.49603256583213806\n",
      "test loss 0.6040779948234558\n",
      "training loss 0.5813695192337036\n",
      "test loss 0.5897613167762756\n",
      "training loss 0.4978538751602173\n",
      "test loss 0.5870625972747803\n",
      "training loss 0.5211918354034424\n",
      "test loss 0.5916043519973755\n",
      "training loss 0.5877325534820557\n",
      "test loss 0.5798472762107849\n",
      "training loss 0.6076747179031372\n",
      "test loss 0.576962947845459\n",
      "training loss 0.5554312467575073\n",
      "test loss 0.5846983790397644\n",
      "training loss 0.5264601707458496\n",
      "test loss 0.5891300439834595\n",
      "training loss 0.5247908234596252\n",
      "test loss 0.581205427646637\n",
      "training loss 0.5859004259109497\n",
      "test loss 0.588079035282135\n",
      "training loss 0.4961678683757782\n",
      "test loss 0.5994199514389038\n",
      "training loss 0.6102775931358337\n",
      "test loss 0.5791202783584595\n",
      "training loss 0.5126484632492065\n",
      "test loss 0.5880104303359985\n",
      "training loss 0.5342167019844055\n",
      "test loss 0.5929341912269592\n",
      "training loss 0.5331324338912964\n",
      "test loss 0.6079025268554688\n",
      "training loss 0.5518549084663391\n",
      "test loss 0.5971692204475403\n",
      "training loss 0.5797768235206604\n",
      "test loss 0.5855209231376648\n",
      "training loss 0.48947349190711975\n",
      "test loss 0.5868794918060303\n",
      "training loss 0.48345303535461426\n",
      "test loss 0.6281601190567017\n",
      "training loss 0.5249082446098328\n",
      "test loss 0.5921756029129028\n",
      "training loss 0.5234512686729431\n",
      "test loss 0.6143092513084412\n",
      "training loss 0.5084695816040039\n",
      "test loss 0.6167550683021545\n",
      "training loss 0.44803690910339355\n",
      "test loss 0.5884129405021667\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+f'nn_clf_hybrid_embedding_{embed_dim}.pt').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8909889840022452\n",
      "AUPRC 0.2792778690674118\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "auroc = metrics.roc_auc_score(y_test,prob)\n",
    "auprc = metrics.average_precision_score(y_test,prob)\n",
    "print(\"AUROC\", auroc)\n",
    "print(\"AUPRC\", auprc)\n",
    "\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'neural network ranking',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 4.52 sec\n",
      "Memory usage: -14.87 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8891734493404434\n",
      "Logit AUPRC 0.2597093707898812\n",
      "XGBoost AUROC 0.8446051431378052\n",
      "XGBoost AUPRC 0.1330079124864457\n",
      "rf AUROC 0.8617474740387314\n",
      "rf AUPRC 0.19421622150832618\n",
      "svm AUROC 0.6585566937973618\n",
      "svm AUPRC 0.19164560326225344\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"Logit AUROC\", auroc)\n",
    "print(\"Logit AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'logistic regression',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost AUROC\", auroc)\n",
    "print(\"XGBoost AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'XGBoost',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"rf AUROC\", auroc)\n",
    "print(\"rf AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'random forest',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})\n",
    "\n",
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "auroc = roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "auprc = average_precision_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "print(\"svm AUROC\", auroc)\n",
    "print(\"svm AUPRC\", auprc)\n",
    "list_dict.append({\n",
    "    'embed_method': 'hybrid',\n",
    "    'embed_dim': embed_dim,\n",
    "    'pred_model': 'support vector machines',\n",
    "    'AUROC': auroc,\n",
    "    'AUPRC': auprc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 4.70 sec\n",
      "Memory usage: -5.35 MB\n"
     ]
    }
   ],
   "source": [
    "# code block to track CPU / Memory usage\n",
    "cpu_sec = time.time() - start_time\n",
    "start_time = time.time()\n",
    "end_memory = psutil.virtual_memory().available\n",
    "mem_use = (start_memory - end_memory) / (1024.0 ** 2)\n",
    "start_memory = end_memory\n",
    "print(f'CPU time: {cpu_sec:.2f} sec')\n",
    "print(f'Memory usage: {mem_use:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
